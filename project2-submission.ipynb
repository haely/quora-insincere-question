{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#importing libraries and reading data\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import seaborn  as sns\n",
    "from sklearn.preprocessing import scale\n",
    "from datetime import datetime\n",
    "pal = sns.color_palette()\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,f1_score, precision_score, recall_score\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input\n",
    "from keras import Model\n",
    "from keras.preprocessing import sequence,text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,SpatialDropout1D,Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer,WordNetLemmatizer\n",
    "stemmer=SnowballStemmer('english')\n",
    "lemma=WordNetLemmatizer()\n",
    "from string import punctuation\n",
    "\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from textblob import Word\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print('# File sizes')\n",
    "for f in os.listdir('../input'):\n",
    "    if 'zip' not in f:\n",
    "        print(f.ljust(30) + str(round(os.path.getsize('../input/' + f) / 1000000, 2)) + 'MB')\n",
    "        \n",
    "print(os.listdir(\"../input\"))\n",
    "        \n",
    "time_record = {}\n",
    "time_record['entire_script'] = {'start': datetime.now()}\n",
    "\n",
    "sample_size = 0\n",
    "submission = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65b56096c4aaed1f1427fb76756196a3c3d5c6de"
   },
   "outputs": [],
   "source": [
    "# reading trainng data\n",
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_train.head()\n",
    "if submission:\n",
    "    df_test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dfcd46ca07b313413a7f759901285eb078f8b789"
   },
   "outputs": [],
   "source": [
    "print('Total number of question for training: {}'.format(len(df_train)))\n",
    "print('Insincere questions: {}%'.format(round(df_train['target'].mean()*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "81d9b4be83379a443690666c7ef63600061f94be"
   },
   "outputs": [],
   "source": [
    "#dropping top n rows from insincere_df, leaving 6000 qs\n",
    "if sample_size > 0:\n",
    "    df_train.drop(df_train.head(len(df_train)-sample_size).index,inplace=True)\n",
    "    if submission:\n",
    "        df_test.drop(df_test.head(len(df_test)-sample_size).index,inplace=True)\n",
    "print(len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "889eb3bb08407a06af8929b3847bb69cba7cd9a4"
   },
   "outputs": [],
   "source": [
    "print('Total number of question for training: {}'.format(len(df_train)))\n",
    "print('Insincere questions: {}%'.format(round(df_train['target'].mean()*100, 2)))\n",
    "insincere = df_train.loc[df_train.target == 1, 'target'].count()\n",
    "sincere = df_train.loc[df_train.target == 0, 'target'].count()\n",
    "print(\"Total number of insincere questions: \", insincere)\n",
    "print(\"Total number of sincere questions: \", sincere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e629c0410ac00820b158ba904c940c533238261e"
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "features = ['word_IDF_norm']\n",
    "def preprocessing(df_train):\n",
    "    start = datetime.now()\n",
    "    # converting to lowercase\n",
    "    df_train['question_text'] = df_train['question_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    print('elapsed time1: ',(datetime.now()-start).total_seconds())\n",
    "    start = datetime.now()\n",
    "\n",
    "    #removing punctuation\n",
    "    df_train['question_text'] = df_train['question_text'].str.replace('[^\\w\\s]','')\n",
    "    print('elapsed time2: ',(datetime.now()-start).total_seconds())\n",
    "    start = datetime.now()\n",
    "\n",
    "    #removing stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    df_train['question_text'] = df_train['question_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    print('elapsed time3: ',(datetime.now()-start).total_seconds())\n",
    "    start = datetime.now()\n",
    "\n",
    "    #lemmatization\n",
    "    df_train['question_text'] = df_train['question_text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "    print('elapsed time4: ',(datetime.now()-start).total_seconds())\n",
    "    start = datetime.now()\n",
    "\n",
    "    #word IDF\n",
    "    word_list = []\n",
    "    df_train['question_text'].apply(lambda x: word_list.extend(list(set(x.split()))))\n",
    "    print('elapsed time5: ',(datetime.now()-start).total_seconds())\n",
    "    start = datetime.now()\n",
    "    word_pd = pd.Series(word_list)\n",
    "    word_row_freq = dict(word_pd.value_counts())\n",
    "    print('elapsed time8: ',(datetime.now()-start).total_seconds())\n",
    "    start = datetime.now()\n",
    "\n",
    "    #start calculating statistics\n",
    "    df_train['word_row_freq'] = df_train['question_text'].apply(lambda x: [word_row_freq[x] for x in x.split()])\n",
    "    print('elapsed time9: ',(datetime.now()-start).total_seconds())\n",
    "    start = datetime.now()\n",
    "    df_train['word_IDF'] = df_train['question_text'].apply(lambda x: [np.log(df_train.shape[0]/word_row_freq[x]) for x in x.split()])\n",
    "    print('elapsed time10: ',(datetime.now()-start).total_seconds())\n",
    "    start = datetime.now()\n",
    "    idf_data = []\n",
    "    df_train['word_IDF'].apply(lambda x: idf_data.extend(x))\n",
    "    print('elapsed time11: ',(datetime.now()-start).total_seconds())\n",
    "    start = datetime.now()\n",
    "    idf_mean = np.mean(idf_data)\n",
    "    idf_var = np.var(idf_data)\n",
    "    print('elapsed time12: ',(datetime.now()-start).total_seconds())\n",
    "    start = datetime.now()\n",
    "    df_train['word_IDF_norm'] = df_train['word_IDF'].apply(lambda x: np.divide(np.subtract(x,idf_mean),idf_var))\n",
    "    print('elapsed time13: ',(datetime.now()-start).total_seconds())\n",
    "    start = datetime.now()\n",
    "    df_train.drop(columns=['word_row_freq','word_IDF'], inplace=True)\n",
    "    print('elapsed time14: ',(datetime.now()-start).total_seconds())\n",
    "\n",
    "    df_train.head()\n",
    "    return df_train\n",
    "    \n",
    "time_record['preprocessing'] = {'start': datetime.now()}\n",
    "df_train = preprocessing(df_train)\n",
    "if submission:\n",
    "    df_test = preprocessing(df_test)\n",
    "time_record['preprocessing']['end'] = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f50774e6f3afb2d8bdca9a46654593c6d574e68e"
   },
   "outputs": [],
   "source": [
    "train, val  = train_test_split(df_train, test_size = 0.1, random_state = 13)\n",
    "if submission:\n",
    "    test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f7ecde2abfd8d1415e6701a66ac3093c8eaab1e"
   },
   "outputs": [],
   "source": [
    "#total unique words\n",
    "time_record['unique_words'] = {'start': datetime.now()}\n",
    "def count_unique_words(pandasdf):\n",
    "    pandasdf.str.lower().str.split()\n",
    "    results = set()\n",
    "    pandasdf.str.lower().str.split().apply(results.update)\n",
    "    uniquewords = len(results)\n",
    "    #print(uniquewords)\n",
    "    return uniquewords\n",
    "\n",
    "# maximum question length\n",
    "def max_question_len(pandasdf):\n",
    "    return max(pandasdf.str.split(\" \").str.len())\n",
    "\n",
    "SERIES_LENGTH = max_question_len(train['question_text'])\n",
    "NUM_UNIQUE_WORDS = count_unique_words(train['question_text'])\n",
    "print(\"Series Length: {}\\nNumber of unique words:{}\".format(SERIES_LENGTH, NUM_UNIQUE_WORDS))\n",
    "time_record['unique_words']['end'] = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c27f090a4e6c6dfa8273dac610ae337c09b485a0"
   },
   "outputs": [],
   "source": [
    "time_record['tokenization_padding'] = {'start': datetime.now()}\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_UNIQUE_WORDS)\n",
    "tokenizer.fit_on_texts(list(train['question_text']))\n",
    "    \n",
    "temp = tokenizer.texts_to_sequences(train['question_text'])\n",
    "train['question_text_padded'] = sequence.pad_sequences(temp, maxlen=SERIES_LENGTH, padding='pre').tolist()\n",
    "for feature in features:\n",
    "    train[feature+\"_padded\"] = sequence.pad_sequences([i for i in train[feature]], maxlen=SERIES_LENGTH, padding='pre', dtype='float32').tolist()\n",
    "    \n",
    "temp = tokenizer.texts_to_sequences(val['question_text'])\n",
    "val['question_text_padded'] = sequence.pad_sequences(temp, maxlen=SERIES_LENGTH, padding='pre').tolist()\n",
    "for feature in features:\n",
    "    val[feature+\"_padded\"] = sequence.pad_sequences([i for i in val[feature]], maxlen=SERIES_LENGTH, padding='pre', dtype='float32').tolist()\n",
    "    \n",
    "if submission:\n",
    "    temp = tokenizer.texts_to_sequences(test['question_text'])\n",
    "    test['question_text_padded'] = sequence.pad_sequences(temp, maxlen=SERIES_LENGTH, padding='pre').tolist()\n",
    "    for feature in features:\n",
    "        test[feature+\"_padded\"] = sequence.pad_sequences([i for i in test[feature]], maxlen=SERIES_LENGTH, padding='pre', dtype='float32').tolist()\n",
    "\n",
    "time_record['tokenization_padding']['end'] = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8beddc107fb8f0e8719ba015991dda7104f4e610"
   },
   "outputs": [],
   "source": [
    "# glove embeddinng, from starter code \n",
    "time_record['embedding'] = {'start': datetime.now()}\n",
    "\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "def get_embed_mat(EMBEDDING_FILE, max_features,embed_dim):\n",
    "    # word vectors\n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf8'))\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    # embedding matrix\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min(max_features, len(word_index) + 1)\n",
    "    all_embs = np.stack(embeddings_index.values()) #for random init\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embed_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    max_features = embedding_matrix.shape[0]\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "gloveEmbed = get_embed_mat('../input/embeddings/glove.840B.300d/glove.840B.300d.txt', NUM_UNIQUE_WORDS, 300)\n",
    "embedding_layer = Embedding(len(tokenizer.word_index)+1,300,weights=[gloveEmbed],input_length=SERIES_LENGTH,trainable=False)\n",
    "time_record['embedding']['end'] = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e748513d41d849ec15f0c2c18d0ca8543edd597c"
   },
   "outputs": [],
   "source": [
    "time_record['augmentation'] = {'start': datetime.now()}\n",
    "def data_augmentation(df_train):\n",
    "    insincere_df = df_train.loc[df_train['target'] == 1]\n",
    "    sincere_df = df_train.loc[df_train['target'] == 0]\n",
    "    insincere1 = insincere_df\n",
    "    insincere2 = insincere_df\n",
    "    insincere3 = insincere_df\n",
    "    insincere4 = insincere_df\n",
    "    insincere5 = insincere_df\n",
    "    fake_insincere = pd.concat([insincere1, insincere2, insincere3, insincere4, insincere5, insincere1, insincere2, insincere3, insincere4, insincere5, insincere1, insincere2, insincere3, insincere4, insincere5], ignore_index=True)\n",
    "    df_quora = pd.concat([sincere_df, fake_insincere], ignore_index=True)\n",
    "    return df_quora\n",
    "\n",
    "train_aug = data_augmentation(train)\n",
    "#basic data info\n",
    "print('Total number of question for training: {}'.format(len(train_aug)))\n",
    "print('Insincere questions: {}%'.format(round(train_aug['target'].mean()*100, 2)))\n",
    "time_record['augmentation']['end'] = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69d9da6562b4aa63a1efcde2594c21d3c095d0a9"
   },
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60e76ea707f0a3bf75540a53eb6a61f16dd3d5b7"
   },
   "outputs": [],
   "source": [
    "time_record['training'] = {'start': datetime.now()}\n",
    "\n",
    "def make_model_b():\n",
    "    feature_num = 1\n",
    "    lstm_out = 100\n",
    "    features = Input(shape=(SERIES_LENGTH, feature_num), dtype='float32', name='features')\n",
    "    word_tokens = Input(shape=(SERIES_LENGTH,), dtype='float32', name='word_tokens')\n",
    "    embedded_words = embedding_layer(word_tokens)\n",
    "    conc = keras.layers.concatenate([embedded_words, features])\n",
    "    lstm = LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.1)(conc)\n",
    "    dens = Dense(1,activation='sigmoid', name='output')(lstm)\n",
    "    \n",
    "    model = Model([word_tokens, features], dens)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = [f1])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "Y_train = train_aug['target'].values\n",
    "X_train_word_tokens = np.array([i for i in train_aug['question_text_padded']])\n",
    "X_train_features = np.array([i for i in train_aug['word_IDF_norm_padded']])\n",
    "\n",
    "X_val_word_tokens = np.array([i for i in val['question_text_padded']])\n",
    "X_val_features = np.array([i for i in val['word_IDF_norm_padded']])\n",
    "Y_val = val['target'].values\n",
    "    \n",
    "X_train_features = np.expand_dims(X_train_features, axis=2)\n",
    "X_val_features = np.expand_dims(X_val_features, axis=2)\n",
    "\n",
    "if submission:\n",
    "    X_test_word_tokens = np.array([i for i in test['question_text_padded']])\n",
    "    X_test_features = np.array([i for i in test['word_IDF_norm_padded']])\n",
    "    X_test_features = np.expand_dims(X_test_features, axis=2)\n",
    "\n",
    "print(X_train_word_tokens.shape, X_train_features.shape, Y_train.shape)\n",
    "print(X_val_word_tokens.shape, X_val_features.shape, Y_val.shape)\n",
    "\n",
    "model = make_model_b()\n",
    "epoch_size = 5\n",
    "if not submission:\n",
    "    history = model.fit([X_train_word_tokens, X_train_features],\n",
    "                    Y_train, batch_size=512,epochs=epoch_size,verbose=1,\n",
    "                    validation_data=[[X_val_word_tokens, X_val_features], Y_val])\n",
    "else:\n",
    "    history = model.fit([X_train_word_tokens, X_train_features],\n",
    "                    Y_train, batch_size=512,epochs=epoch_size,verbose=1)\n",
    "\n",
    "time_record['training']['end'] = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6fafc823152a8b4d0808768b5a1aa86acd28d2de"
   },
   "outputs": [],
   "source": [
    "if not submission:\n",
    "    plt.plot(history.history['val_f1'])\n",
    "plt.plot(history.history['f1'])\n",
    "for stat in history.history:\n",
    "    print(stat,history.history[stat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3dc29d5093fb0d9ebfd2cb0025443c93eecdd36b"
   },
   "outputs": [],
   "source": [
    "def line_search_f1_score(y_score, y_test):\n",
    "    max_f1_score = 0\n",
    "    opt_threshold = 0\n",
    "    for threshold in [i*0.01 for i in range(100)]:\n",
    "        y_preds = y_score > threshold\n",
    "        score = f1_score(y_preds, y_test)\n",
    "        if max_f1_score < score:\n",
    "            max_f1_score = score\n",
    "            opt_threshold = threshold\n",
    "    return max_f1_score, opt_threshold\n",
    "\n",
    "time_record['threshold'] = {'start': datetime.now()}\n",
    "\n",
    "y_score_val = model.predict([X_val_word_tokens, X_val_features], verbose=1)\n",
    "max_f1_score, threshold = line_search_f1_score(y_score_val, Y_val)\n",
    "    \n",
    "print(threshold)\n",
    "print(max_f1_score)\n",
    "\n",
    "time_record['threshold']['end'] = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2580f9303e22ee32e1255fd1bcc94b9f5350eab3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if submission:\n",
    "    time_record['submission'] = {'start': datetime.now()}\n",
    "\n",
    "    y_score_test = model.predict([X_test_word_tokens, X_test_features], verbose=1)\n",
    "\n",
    "    df_test['prediction'] = np.array(y_score_test > threshold, dtype=int)\n",
    "    df_test[['qid','prediction']].to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "    time_record['submission']['end'] = datetime.now()\n",
    "time_record['entire_script']['end'] = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9154947162d79072ae70d5847ec431b81bbef81"
   },
   "outputs": [],
   "source": [
    "for subsection in time_record:\n",
    "    print(\"{}: {}\".format(subsection, (time_record[subsection]['end']-time_record[subsection]['start']).total_seconds()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
